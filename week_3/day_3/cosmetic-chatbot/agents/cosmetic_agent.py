import openai
import time
from datetime import datetime, timedelta
from config.settings import Config
from prompts.system_prompts import SystemPrompts
from tools.product_search import ProductSearch
from tools.skin_analyzer import SkinAnalyzer
from tools.order_tracker import OrderTracker
from monitoring.cost_monitor import cost_monitor  # üìä IMPORT NOUVEAU

class CosmeticAgentGPT4:
    """Agent optimis√© pour GPT-4.1 avec monitoring des co√ªts"""
    
    def __init__(self):
        self.config = Config()
        self.prompts = SystemPrompts()
        self.product_tool = ProductSearch()
        self.skin_tool = SkinAnalyzer()
        self.order_tool = OrderTracker()
        
        # üîß Configuration OpenRouter
        openai.api_base = self.config.API_BASE
        openai.api_key = self.config.OPENROUTER_API_KEY
        
        # üí∞ Gestion des co√ªts et limites
        self.request_times = []
    
    def _check_rate_limits(self):
        """V√©rifie les limites de taux et de co√ªt"""
        now = datetime.now()
        
        # üö´ V√©rification autorisation via cost_monitor
        can_request, message = cost_monitor.can_make_request()
        if not can_request:
            raise Exception(message)
        
        # ‚è±Ô∏è V√©rification limite de taux
        self.request_times = [t for t in self.request_times if now - t < timedelta(minutes=1)]
        if len(self.request_times) >= self.config.REQUESTS_PER_MINUTE:
            time.sleep(10)  # Pause si trop de requ√™tes
    
    def _generate_llm_response(self, prompt: str, context: dict = None, user_message: str = "") -> str:
        """G√©n√®re une r√©ponse avec monitoring des co√ªts"""
        
        # üõ°Ô∏è V√©rification des limites
        self._check_rate_limits()
        self.request_times.append(datetime.now())
        
        try:
            # üéØ Pr√©paration du message avec contexte riche
            messages = [
                {"role": "system", "content": self.prompts.COSMETIC_EXPERT_GPT4},
            ]
            
            # üìù Ajout du contexte si disponible
            if context:
                context_message = f"""
                CONTEXTE SUPPL√âMENTAIRE :
                - Type de peau : {context.get('skin_type', 'Non sp√©cifi√©')}
                - √Çge estim√© : {context.get('age_range', 'Non sp√©cifi√©')}
                - Pr√©occupations : {context.get('concerns', [])}
                - Produits disponibles : {context.get('available_products', [])}
                """
                messages.append({"role": "system", "content": context_message})
            
            messages.append({"role": "user", "content": prompt})
            
            # üöÄ Appel GPT-4.1 avec param√®tres optimis√©s
            response = openai.ChatCompletion.create(
                model=self.config.DEFAULT_MODEL,
                messages=messages,
                max_tokens=self.config.MAX_TOKENS,
                temperature=0.7,
                top_p=0.9,
                frequency_penalty=0.1,
                presence_penalty=0.1,
                headers=self.config.HEADERS
            )
            
            # üí∞ Tracking des co√ªts AVEC MONITORING
            tokens_used = response.usage.total_tokens
            cost = self._estimate_cost(self.config.DEFAULT_MODEL, tokens_used)
            
            # üìä ENREGISTREMENT DANS LE MONITOR
            cost_monitor.log_usage(
                model=self.config.DEFAULT_MODEL,
                tokens=tokens_used,
                cost=cost,
                user_message=user_message
            )
            
            print(f"‚úÖ GPT-4.1 utilis√© - Tokens: {tokens_used} - Co√ªt: {cost:.4f}‚Ç¨")
            
            return response.choices[0].message.content.strip()
            
        except openai.error.RateLimitError:
            return "‚ö†Ô∏è Service temporairement satur√©. Veuillez r√©essayer dans quelques instants."
        except openai.error.APIError as e:
            return f"‚ùå Erreur API: {str(e)}"
        except Exception as e:
            return f"üîß Erreur technique: {str(e)}"
    
    def _estimate_cost(self, model: str, tokens: int) -> float:
        """Estime le co√ªt d'une requ√™te"""
        cost_per_token = self.config.MODEL_COSTS.get(model, 0.03) / 1000
        return tokens * cost_per_token

    # ... [le reste du code reste identique] ...